{"cells":[{"cell_type":"code","execution_count":15,"id":"bf199e6a","metadata":{"id":"32b3ec57","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":16,"id":"5ac36d3a","metadata":{"id":"c0ccf76b","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cf88b954-f39a-412a-d87e-660833e735b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-125b  GCE       2                                             RUNNING  us-central1-a\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security â†’ Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"01ec9fd3"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":17,"id":"d8f56ecd","metadata":{"id":"5609143b","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":18,"id":"38a897f2","metadata":{"id":"b10cc999","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8f93a7ec-71e0-49c1-fc81-9af385849a90"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Mar 12 00:17 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":19,"id":"47900073","metadata":{"id":"d3f86f11","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":20,"id":"72bed56b","metadata":{"id":"5be6dc2a","nbgrader":{"grade":false,"grade_id":"cell-spark-version","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"07b4e22b-a252-42fb-fe46-d9050e4e7ca8","scrolled":true},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-125b-m.c.irproject24.internal:37417\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7fafbeefa800>"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":21,"id":"980e62a5","metadata":{"id":"7adc1bf5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["multistream10_preprocessed.parquet\n","multistream11_part2_preprocessed.parquet\n","multistream11_preprocessed.parquet\n","multistream12_part2_preprocessed.parquet\n","multistream12_preprocessed.parquet\n","multistream13_part2_preprocessed.parquet\n","multistream13_preprocessed.parquet\n","multistream14_part2_preprocessed.parquet\n","multistream14_preprocessed.parquet\n","multistream15_part2_preprocessed.parquet\n","multistream15_part3_preprocessed.parquet\n","multistream15_preprocessed.parquet\n","multistream16_part2_preprocessed.parquet\n","multistream16_part3_preprocessed.parquet\n","multistream16_preprocessed.parquet\n","multistream17_part2_preprocessed.parquet\n","multistream17_part3_preprocessed.parquet\n","multistream17_preprocessed.parquet\n","multistream18_part2_preprocessed.parquet\n","multistream18_part3_preprocessed.parquet\n","multistream18_preprocessed.parquet\n","multistream19_part2_preprocessed.parquet\n","multistream19_part3_preprocessed.parquet\n","multistream19_preprocessed.parquet\n","multistream1_preprocessed.parquet\n","multistream20_part2_preprocessed.parquet\n","multistream20_part3_preprocessed.parquet\n","multistream20_preprocessed.parquet\n","multistream21_part2_preprocessed.parquet\n","multistream21_part3_preprocessed.parquet\n","multistream21_preprocessed.parquet\n","multistream22_part2_preprocessed.parquet\n","multistream22_part3_preprocessed.parquet\n","multistream22_part4_preprocessed.parquet\n","multistream22_preprocessed.parquet\n","multistream23_part2_preprocessed.parquet\n","multistream23_part3_preprocessed.parquet\n","multistream23_part4_preprocessed.parquet\n","multistream23_preprocessed.parquet\n","multistream24_part2_preprocessed.parquet\n","multistream24_part3_preprocessed.parquet\n","multistream24_part4_preprocessed.parquet\n","multistream24_part5_preprocessed.parquet\n","multistream24_preprocessed.parquet\n","multistream25_part2_preprocessed.parquet\n","multistream25_part3_preprocessed.parquet\n","multistream25_part4_preprocessed.parquet\n","multistream25_preprocessed.parquet\n","multistream26_preprocessed.parquet\n","multistream27_part2_preprocessed.parquet\n","multistream27_part3_preprocessed.parquet\n","multistream27_preprocessed.parquet\n","multistream2_preprocessed.parquet\n","multistream3_preprocessed.parquet\n","multistream4_preprocessed.parquet\n","multistream5_preprocessed.parquet\n","multistream6_preprocessed.parquet\n","multistream7_preprocessed.parquet\n","multistream8_preprocessed.parquet\n","multistream9_preprocessed.parquet\n"]}],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = '322514282' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if \"parquet\" in b.name:\n","        print(b.name)\n","        paths.append(full_path+b.name)"]},{"cell_type":"markdown","id":"cac891c2","metadata":{"id":"13ZX4ervQkku"},"source":["***GCP setup is complete!*** If you got here without any errors you've earned 10 out of the 35 points of this part."]},{"cell_type":"markdown","id":"481f2044","metadata":{"id":"02f81c72"},"source":["Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."]},{"cell_type":"code","execution_count":22,"id":"e4c523e7","metadata":{"id":"b1af29c9"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","doc_title_pairs = parquetFile.select(\"id\", \"title\").rdd"]},{"cell_type":"markdown","id":"0d7e2971","metadata":{"id":"f6375562"},"source":["We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"]},{"cell_type":"code","execution_count":23,"id":"82881fbf","metadata":{"id":"d89a7a9a"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["6348910"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# Count number of wiki pages\n","parquetFile.count()"]},{"cell_type":"code","execution_count":24,"id":"f3ad8fea","metadata":{"id":"a4b6ee29","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)"]},{"cell_type":"code","execution_count":25,"id":"55c8764e","metadata":{"id":"0b5d7296","nbgrader":{"grade":false,"grade_id":"cell-index_construction","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["dt_dict = doc_title_pairs.collectAsMap()"]},{"cell_type":"code","execution_count":26,"id":"ab3296f4","metadata":{"id":"Opl6eRNLM5Xv","nbgrader":{"grade":true,"grade_id":"collect-posting","locked":true,"points":0,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["dt_clean = 'dt.pkl'\n","with open(dt_clean, 'wb') as f:\n","    pickle.dump(dt_dict, f)\n","\n","bucket = client.bucket(bucket_name)\n","blob = bucket.blob(f\"dt/{dt_clean}\")\n","\n","blob.upload_from_filename(dt_clean)"]},{"cell_type":"code","execution_count":27,"id":"3d87c0d6","metadata":{},"outputs":[{"data":{"text/plain":["6348910"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["len(dt_dict)"]},{"cell_type":"code","execution_count":null,"id":"48f8a21d","metadata":{},"outputs":[],"source":[]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"collapsed_sections":[],"name":"assignment3_gcp.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}